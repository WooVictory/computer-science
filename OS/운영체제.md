# 운영체제

### 📖 **목차**
* [(멀티)프로세스와 (멀티)스레드](#멀티프로세스와-멀티스레드)
    * [프로세스](#프로세스)
    * [프로세스의 상태](#프로세스의-상태)
    * [스레드](#스레드)
    * [멀티 프로세스](#멀티-프로세스)
    * [멀티 스레드](#멀티-스레드)
* [프로세스 제어 블록](#프로세스-제어-블록)
    * [PCB에 저장되는 정보](#PCB에-저장되는-정보)
* [문맥교환(Context Switching)](#문맥교환Context-Switching)
    * [문맥 교환이 일어나는 경우](#문맥-교환이-일어나는-경우)
* [동기화란](#동기화란)
* [데드락과 발생 조건](#데드락과-발생-조건)
    * [데드락](#데드락)
    * [발생 조건](#발생-조건)
    * [해결 방법](#해결-방법)
* [임계영역 접근 동기화](#임계영역-접근-동기화)
* [세마포어, 뮤텍스, 모니터의 차이](#세마포어-뮤텍스-모니터의-차이)
    * [세마포어](#세마포어)
    * [뮤텍스](#뮤텍스)
    * [모니터](#모니터)
    * [뮤텍스와 모니터의 차이점](#뮤텍스와-모니터의-차이점)
    * [세머포어와 모니터의 차이점](#세머포어와-모니터의-차이점)
* [CPU 스케줄링](#CPU-스케줄링)
    * [Preemptive vs Non-Preemptive](#Preemptive-vs-Non-Preemptive)
    * [선점형 스케줄링](#선점형-스케줄링)
    * [비선점형 스케줄링](#비선점형-스케줄링)
* [스케줄러의 종류](#스케줄러의-종류)
    * [장기 스케줄러](#장기-스케줄러)
    * [단기 스케줄러](#단기-스케줄러)
    * [중기 스케줄러](#중기-스케줄러)
    * [정리](#정리)
* [시스템 콜(System Call)](#시스템-콜System-Call)
    * [시스템 콜의 유형](#시스템-콜의-유형)
* [인터럽트](#인터럽트)
    * [인터럽트의 종류](#인터럽트의-종류)
    * [인터럽트 발생 처리 과정](#인터럽트-발생-처리-과정)
* [IPC(Inter-Process-Communication)](#IPCInter-Process-Communication)
* [메모리](#메모리)
* [페이징과 세그멘테이션](#페이징과-세그멘테이션)
* [단편화](#단편화)
    * [외부 단편화](#외부-단편화)
    * [내부 단편화](#내부-단편화)
    * [압축(Compaction)](#압축Compaction)
* [Swapping](#Swapping)
* [페이지 교체 알고리즘](#페이지-교체-알고리즘)
* [파일 시스템](#파일-시스템)
* [동기와 비동기](#동기와-비동기)
* [Blocking Non-blocking](#Blocking-Non-blocking)
* [캐시의 지역성](#캐시의-지역성)

---

<br>

## (멀티)프로세스와 (멀티)스레드

### 프로세스
프로세스란 수행중인 프로그램을 뜻하며, 수행중이란 뜻은 실행 파일 형태로 존재하던 프로그램이 메모리에 올라가 수행되기 시작하는 것을 말한다. 이러한 프로세스는 CPU를 획득하여 자신의 코드를 수행하거나 CPU를 반환하고 입출력 작업을 수행하기도 한다.

할당받는 시스템 자원
- CPU 시간
- 운영되기 위한 주소 공간
- Code, Data, Stack, Heap의 구조로 되어있는 독립된 메모리 영역.

<img src="https://user-images.githubusercontent.com/33534771/77537773-fe37fb00-6ee1-11ea-8def-4dd11523b5e7.png" />

- 기본적으로 프로세스마다 최소 1개의 스레드를 갖는다.(메인 스레드)
- 프로세스는 각각 별도의 메모리 영역(주소 공간)을 할당받는다.  [Code, Data, Stack, Heap]
- 한 프로세스는 다른 프로세스의 변수나 자료구조에 접근할 수 없으며, 접근을 위해서는 IPC 통신이 필요하다.
  - Ex) 파이프, 파일, 소켓 등을 이용한 통신 방법 이용.

### 프로세스의 상태

- 실행(Running) : 프로세스가 CPU를 보유하고 기계어 명령을 실행하고 있는 상태

- 준비(Ready) : 프로세스가 CPU만 보유하면 당장이라고 명령을 실행할 수 있지만, CPU를 할당받지 못한 상태

- 봉쇄(Blocked, Wait, Sleep) : 프로세스에게 CPU를 할당해 주어도 당장 명령을 실행할 수 없는 상태

- 시작(New) :  프로세스가 시작되어 프로세스를 위한 각종 자료 구조는 생성되었지만 아직 메모리 획득을 승인받지 못한 상태

- 완료(Terminated) : 프로세스가 종료되었으나 운영 체제가 프로세스와 관련된 자료 구조를 완전히 정리하지 못한 상태

![](https://i.imgur.com/JDADmx1.png)



### 스레드

프로세스 안에서 실행되는 여러 흐름 단위로 **스택영역만 따로 할당**받고 나머지 영역은 서로 공유하게 된다.

스레드는 스레드 ID, 프로그램 카운터, 레지스터 집합, 그리고 스택으로 구성된다.

> 스레드마다 스택 영역을 따로 할당받는 이유?
>
> * 스택은 함수 호출시 전달되는 인자, 되돌아갈 주소값, 함수 내에서 선언하는 변수 등을 저장하기 위해 사용되는 메모리 공간이므로 스택 메모리 공간이 독립적이라는 것은 독립적인 함수 호출이 가능하다는 것을 뜻한다. 즉, 스레드의 정의처럼 **독립적인 실행 흐름을 추가하기 위해** 스레드는 독립적인 스택 공간을 사용하게 된다.

`요약`
프로세스는 자신만의 고유 공간과 자원을 할당받아 사용하는데 반해 스레드는 다른 스레드와 공간, 자원을 공유하면서 사용하는 차이가 있다.



### 멀티 프로세스

- 하나의 응용프로그램을 여러 개의 프로세스로 구성하여 각 프로세스가 하나의 작업을 처리하도록 하는 것.
- 장점
  - 여러 개의 자식 프로세스 중 하나에 문제가 발생하면 그 자식 프로세스만 죽는 것 이상으로 다른 영향이 확산되지 않는다. (안전성)
- 단점
  1. Context Switching에서의 오버헤드
     - 프로세스는 각 독립된 메모리 영역을 할당받았기 때문에 공유하는 메모리가 없다. 따라서 캐시 메모리 초기화 등의 무거운 작업이 진행되고 많은 시간이 소모되는 등의 오버헤드가 발생할 문제가 있다.
  2. 프로세스 간 통신 기법 IPC
     - 프로세스는 각 독립된 메모리 영역을 할당받았기 때문에 프로세스들 사이에서 변수나 자료구조를 공유할 수 없다. 따라서 IPC라는 방법을 사용해야 하며, 이는 어렵고 복잡한 통신 방법이다.



### 멀티 스레드

- 하나의 응용 프로그램을 여러 개의 스레드로 구성하고 각 스레드가 하나의 작업을 처리하도록 하는 것.
- 윈도우, 리눅스 등 많은 OS들이 멀티 프로세싱을 지원하고 있지만, 멀티 스레딩을 기본으로 하고 있다.
- 웹 서버는 대표적인 멀티 스레드 응용 프로그램이다.
- 장점
  - 메모리 공간과 시스템 자원 소모가 줄어들게 된다.
  - 스레드 간 통신시, 전역 변수의 공간 또는 동적으로 할당된 공간인 Heap 영역을 이용해 데이터를 주고 받으므로 통신 방법이 간단하다.
  - Context switching 시, 캐시 메모리를 비울 필요가 없기 때문에 비용이 적고 더 빠르다.
  - 따라서 시스템의 처리량이 향상되고 자원 소모가 줄어들며, 자연스럽게 프로그램의 응답 시간이 단축된다.

- 단점
  - 서로 다른 스레드가 Data, Heap 영역 등을 공유하기 때문에 어떤 스레드가 다른 스레드에서 사용중인 변수나 자료구조에 접근하여 엉뚱한 값을 읽어오거나 수정할 수 있다. 즉, 자원 공유의 문제가 발생한다.(동기화)
  - 하나의 스레드에 문제가 생기면 전체 프로세스가 영향을 받는다.
  - 주의 깊은 설계가 필요하며, 디버깅이 까다롭다


`요약`

멀티 스레드는 멀티 프로세스보다 적은 메모리 공간을 차지하고 Context Switching이 빠르다는 장점이 있지만, 오류로 인해 하나의 스레드가 종료되면 전체 스레드가 종료될 수 있다는 점과 동기화 문제를 가지고 있다. 

반면, 멀티 프로세싱 방식은 하나의 프로세스가 죽더라도 다른 프로세스에는 영향을 끼치지 않고 정상적으로 수행된다는 장점이 있지만, 멀티 스레드보다 많은 메모리 공간과 CPU 시간을 차지한다는 단점이 존재한다. 

이 두 가지는 동시에 여러 작업을 수행한다는 점에서 같지만 적용해야 하는 시스템에 따라 적합/부적합이 구분된다. 따라서 대상 시스템의 특징에 따라 적합한 동작 방식을 선택하고 적용해야 한다



## 프로세스 제어 블록

PCB는 특정 **프로세스에 대한 중요한 정보를 저장** 하고 있는 운영체제의 자료구조이다.

운영체제는 프로세스를 관리하기 위해 **프로세스의 생성과 동시에 고유한 PCB를 생성**한다.

프로세스는 CPU를 할당받아 작업을 처리하다가도 프로세스 전환이 발생하게 되면 진행하던 작업을 저장하고 CPU를 반환해야 하는데, 이때 작업의 진행 상황을 모두 PCB에 저장하게게 된다. 그리고나서 다시 CPU를 할당받게 되면 PCB에 저장되어 있던 내용을 불러와 이전에 종료됐던 시점부터 다시 작업을 수행하게 된다.

### PCB에 저장되는 정보

* 프로세스 식별자(PID ,Process ID): 프로세스 식별 번호
* 프로세스 상태: new, ready, running, waiting, terminated 등의 상태를 저장
* 프로그램 카운터: 프로세스가 다음에 실행할 명령어의 주소
* CPU 스케줄링 정보: 프로세스의 우선순위, 스케줄 큐에 대한 포인터 등
* 메모리 관리 정보: 페이지 테이블 또는 세그먼트 테이블 등과 같은 정보를 포함
* 입출력 상태 정보: 프로세스에 할당된 입출력 장치들과 열린 파일 목록
* 어카운팅 정보: 사용된 CPU 시간, 시간제한, 계정 번호 등

![](https://i.imgur.com/CB3VBoz.png)


## 문맥교환(Context Switching)

간단히 말하자면 프로세스의 상태를 변경하는 것이다. 스케줄링에 의해 실행 중인 코드나 자원 등을 저장하고, 현재 상태를 대기 상태로 만들고 다른 프로세스를 실행시키는 과정이다. 멀티 프로세스 환경에서 CPU가 어떤 하나의 프로세스를 실행하고 있는 상태에서 인터럽트 요청에 의해 다음 우선 순위의 프로세스가 실행되어야 할 때, 기존의 프로세스 상태 또는 레지스터 값(Context)를 저장하고, CPU가 다음 프로세스를 수행하도록 새로운 프로세스의 상태 또는 레지스터 값을 교체하는 작업이다.

System Call이나 Interrupt가 발생하면 CPU의 제어권이 운영 체제에게로 넘어와 원래 실행중이던 프로세스의 업무를 잠시 멈추고 운영체제 커널의 코드가 실행된다. 해당 경우에도 CPU의 실행 위치 등 프로세스의 문맥 중 일부를 PCB에 저장하게 되지만, 이는 문맥 교환이 아니다. **그 이유는 하나의 프로세스가 사용자 모드에서 실행되다가 커널 모드로 실행 모드만 변경한 것일 뿐 CPU를 점유하는 프로세스가 다른 사용자 프로세스로 변경되는 과정이 아니기 때문이다.**

![](https://i.imgur.com/uWWMPyI.png)
 [문맥 교환이 일어나는 경우와 그렇지 않는 경우의 예시]

문맥 교환이 일어나는 시간은 시스템 입장에서 볼 때 일종의 오버헤드라고 할 수 있다. 만약 타이머를 줄여 CPU 할당 시간을 줄이면 문맥의 교환이 빈번해지기 때문에 오버헤드가 상당히 커지게 되고, 그 반대로 할당 시간을 크게 늘려버리면 시분할 시스템의 의미가 퇴색되어버린다.따라서 적절한 CPU 할당 시간을 정하는 것이 중요하다.


### 문맥 교환이 일어나는 경우
- I/O request : 입출력 요청
- time slice expired : CPU 사용 시간이 만료
- fork a child : 자식 프로세스 생성
- wait for an interrupt : 인터럽트 처리 대기

## 동기화란?

- 프로세스 또는 스레드들이 수행되는 시점을 조절하여 서로가 알고 있는 정보가 일치하는 것을 의미한다.

컴퓨터는 프로세스 또는 스레드로 작업을 실행한다. 예를 들어, a = 10이라는 자원이 있다고 가정해보자.

P1이라는 프로세스는 a라는 값을 이용하여 5를 더하는 로직을 수행한다.

그런데 만약, 그 사이 P2라는 프로세스가 a의 값을 20으로 바꾸게 된다면 어떻게 될까?

값은 P1이 수행한 15가 되는 것일까, P2가 20으로 바꾸고 P1이 5를 더해서 25가 되는 걸까?

-> 아마 정확한 값을 반환하지 않을 것이다.



위에서 a를 **공유 자원** 이라 말한다.

이렇게 두 개 이상의 프로세스 혹은 스레드가 동기화 없이 접근하려는 현상을 race condition(경쟁 상태)라고 한다.

그리고 이러한 문제를 해결하는 것, 즉, 서로가 알고 있는 정보를 일치시키려는 것을 synchronization(동기화)라고 한다.



## 임계영역 접근 동기화

#### 임계 영역(critical section)

둘 이상의 스레드가 동시에 실행될 경우 동시 접근 문제를 발생시킬 수 있는 코드 블록이며, **공유되는 자원에서 문제가 발생하지 않도록 독점을 보장해주는 영역이다.**



따라서 임계 영역에서는 한번에 하나의 쓰레드만 사용 가능하다. 프로세스 내에 여러 스레드가 있는 환경에서, 임계 영역에 어떤 스레드가 먼저 진입하여 임계 영역을 벗어나지 않은 상태에서는 동일 프로세스의 다른 스레드가 해당 임계영역에 접근하는 것을 금지한다.



따라서 임계 영역에 동시 접근하게 되면 발생하는 문제를 방지하기 위해 동기화를 위한 방법이 필요하다.



<!-- 임계 영역(critical section)이란, 둘 이상의 쓰레드가 동시에 실행될 경우 동시 접근 문제를 발생시킬 수 있는 코드 블록을 말한다. 따라서 임계 영역에서는 한 번에 하나의 '쓰레드'에서만 사용이 가능하다. 프로세스 내에 여러 쓰레드가 있는 환경에서, 설정해둔 크리티컬 영역에 어떤 쓰레드가 먼저 진입하여 크리티컬 영역을 벗어나지 않은 상태에서는 동일 프로세스의 다른 쓰레드가 해당 크리티컬 영역에 진입하는 것을 금지한다. 후발 쓰레드의 '크리티컬 섹션' 진입금지 방식에 '리턴' 혹은 '대기'를 설정할 수 있다. '대기'란 진입해있던 쓰레드가 해당 영역을 벗어날 때까지 후발 쓰레드는 '대기' 상태로 있다가,선진입한 쓰레드가 해당영역에서 벗어나면 대기중인 후발 쓰레드가 크리티컬 영역을 실행하는 것이다.'리턴' 이란 후발 쓰레드는 '대기' 상태에 있지 않고, '리턴'되어 크리티컬 영역을 실행하지 않는 방식이다.정리하자면 임계영역 접근 동기화란 동시 접근을 하게되면 발생하는 문제를 예방하기 위해 동시 접근을 예방하는 동시에 한 번에 하나의 쓰레드만 임계영역에 접근할 수 있도록 하는 것이다. -->

## 세마포어, 뮤텍스, 모니터의 차이

### 세마포어(Semaphore)

- 공유된 자원의 데이터를 여러 프로세스 혹은 스레드가 접근하는 것을 막는 동기화 기법이다.

- 세마포어는 신호등이라는 뜻이며, 두 가지 연산으로 이루어져 있다.
- 임계 영역의 사용이 끝났으면 사용이 끝난 프로스세스가 대기 중인 프로세스에게 사용이 끝났다는 사실을 알려주는 방식이다. 
- 동시에 접근할 수 있는 '허용 가능한 갯수'를 가지고 있는 일종의 Counter이다. 공통으로 관리되는 정수값이며, 공유자원에 접근할 수 있는 스레드 혹은 프로세스의 수를 나타낸다.
- semSignal : 세마포어 값을 증가시킨다. 만약 값이 0이거나 음수일 경우에는 semwait 연산으로 블록된 프로세스를 깨운다.
- 세마포어의 갯수에 따라 다음과 같이 나뉜다.
  - 1개 : Binary Semaphore(뮤텍스와 동일하다.)
  - 2개 이상 : Counting Semaphore
- 세마포어는 소유할 수 없다. 



### 뮤텍스(Mutex)

- 마찬가지로, 공유된 자원의 데이터를 여러 프로세스 혹은 스레드가 접근하는 것을 막는 동기화 기법이다.
- 다만, 동기화 대상이 오직 하나이며 이종의 Locking 매커니즘으로 Lock을 가지고 있을 경우에만 공유 데이터에 접근이 가능하다.
- Lock에 대한 소유권이 있으며 Lock을 가지고 있을 경우에만 공유 자원에 접근할 수 있고, Lock을 가진 사람만 반납할 수 있다. 
- **뮤텍스 객체를 두 스레드가 동시에 사용할 수 없다.**
- Ex) 공유 화장실을 예로 들어보자. 1개의 화장실이 있으며, 열쇠(Lock)이 있어야 사용할 수 있다. 열쇠를 가진 사람만이 화장실에 갈 수 있고, 다음 사람이 화장실에 가기 위해서는 앞사람이 열쇠를 반납해야 한다. 즉, Lock에 대한 소유권이 있어야 한다.



### 모니터(Monitor)

모니터는 **공유 자원 + 공유 자원 접근함수**로 이루어져 있고, 2개의 큐를 가지고 있다. 각각 **mutual exclusion(상호배제) queue, conditional synchronization(조건동기) queue**이다.

> * 상호배제 큐는 말그대로 공유 자원에 하나의 프로세스만 진입하도록 하기 위한 큐이다.
> * 조건동기 큐는 이미 공유자원을 사용하고 있는 프로세스가 특정한 호출  `wait()`을 통해 조건동기 큐로 들어갈 수 있다.

조건동기 큐에 들어가 있는 프로세스는 공유자원을 사용하고 있는 다른 프로세스에 의해 깨워줄 수 있다. 이 역시 깨워주는 프로세스에서 특정한 호출 `notify()`을 해주며, 깨워주더라도 이미 공유자원을 사용하고 있는 프로세스가 해당 구역을 나가야 비로소 큐에 있던 프로세스가 실행된다.



### 뮤텍스와 모니터의 차이점

- 뮤텍스는 다른 프로세스나 스레드 간에 동기화를 위해 사용한다.
- 모니터는 하나의 프로세스 내에서 다른 스레드 간의 동기화를 위해 사용한다.
- 뮤텍스는 운영체제 커널에 의해 제공된다.
  - 무겁고 느리다.
- 모니터는 프레임워크나 라이브러리 그 자체에서 제공된다.
  - 가볍고 빠르다.

### 세마포어와 모니터의 차이점

- 자바에서는 모니터를 모든 객체에게 기본적으로 제공하지만, C에서는 사용할 수 없다.
- 세마포어는 카운터라는 변수값으로 프로그래머가 상호 배제나 정렬의 목적으로 사용시 매번 값을 따로 지정해줘야 하는 등 조금 번거롭다.
- 반면, 모니터는 이러한 일들이 캡슐화되어 있어 개발자는 카운터 값을 0 또는 1로 주어야 하는 고민을 할 필요가 없다. synchronized, wait(), notify() 등의 키워드를 이용해 좀 더 편하게 동기화할 수 있다.



#### 세마포어와 뮤텍스의 차이는?

- 세마포어는 뮤텍스가 될 수 있지만, 뮤텍스는 세마포어가 될 수 없다.
- 세마포어는 소유할 수 없으며, 뮤텍스는 소유할 수 있고 소유주가 그 책임을 진다.
- 뮤텍스의 경우 뮤텍스를 소유하고 있는 스레드가 이 뮤텍스를 해제할 수 있다. 하지만 세마포어는 소유하지 않고 있는 다른 스레드가 세마포어를 해제할 수 있다.
- 뮤텍스는 동기화 대상이 1개일 때 사용하고 세마포어는 동기화 대상이 여러 개일때 사용한다.



## 데드락과 발생 조건

### 데드락
프로세스가 자원을 얻지 못해 다음 처리를 하지 못하는 상태로 '교착 상태' 라고도 부른다. 시스템적으로 한정된 자원을 여러 곳에서 사용하려고 할 때 발생하게 된다. `프로세스 1`이 `A 자원`을 얻고 `프로세스 2` 가 `B 자원` 을 얻은 상태에서 서로의 자원이 필요하게 되었을 때에 `프로세스 1` 은 `B 자원` 을 얻기 위해 대기하고 `프로세스 2` 는 `A 자원` 을 얻기위해 기다리게 되며 서로를 무한정 기다리게 되는 것을 데드락이라고 한다. 주로 멀티 프로그래밍 환경에서 발생하게 된다.

### 발생 조건

* **상호 배제**: 자원은 한번에 한 프로세스만 사용할 수 있다.
* **점유 대기**: 최소한 하나의 자원을 점유하고 있으면서 다른 프로세스에서 할당되어 사용하고 있는 자원을 추가로 점유하기 위해 대기하는 프로세스가 존재해야 한다.
* **비선점**: 다른 프로세스에 할당된 자원은 사용이 끝날 때 까지 강제로 빼앗을 수 없다.
* **순환 대기**: 프로세스의 집합에서 순환 형태로 자원을 대기하고 있어야 한다.

### 해결 방법

- 예방
  - 발생 조건 네가지를 동시에 충족해야 데드락이 발생하기 때문에 그 중 하나라도 발생하지 않도록 시스템 차원에서 막아버리면 해결
  - 그런데 이 방법은 대부분 자원이 낭비되는 경향이 있다. 애초에 발생 가능성을 원천봉쇄 하려면 성능이 나빠지거나 또 다른 문제를 야기할 수도 있음
- 회피
  - 은행원 알고리즘
    - 프로세스가 자원을 요구하는 시점에 자원을 할당해도 안전한지를 검사하여 데드락을 막는 방법
- 탐지와 회복
  - 교착상태가 발생하는 것을 아예 막지조차 않음. 발생하면 그때서야 해결
- 무시
  - 거의 무시해도 좋을 확률로 데드락이 발생한다고 판단되면 그냥 무시
  - 왜냐하면 데드락 문제는 해결하려면 성능상 손해를 봐야하기 때문

## CPU 스케줄링

CPU가 하나의 프로세스 작업이 끝나면 다음 프로세스 작업을 수행해야 한다. 이때 어떤 프로세스를 다음에 처리할 지 선택하는 알고리즘을 CPU Scheduling 알고리즘이라고 한다. 따라서 상황에 맞게 CPU를 어떤 프로세스에 배정하여 효율적으로 처리하는가가 관건이다.


### Preemptive vs Non-Preemptive

1) Preemptive(선점)

- 프로세스가 CPU를 점유하고 있는 동안 I/O나 인터럽트가 발생하지 않았음에도 다른 프로세스가 해당 CPU를 강제로 점유할 수 있다. 
- 즉, 프로세스가 정상적으로 수행중인 동안 다른 프로세스가 CPU를 강제로 점유하여 실행할 수 있다.



2) Non-Preemptive(비선점)

- 한 프로세스가 CPU를 점유했다면 I/O나 인터럽트 발생 또는 프로세스가 종료될 때까지 다른 프로세스가 CPU를 점유하지 못하는 것이다. 


### 선점형 스케줄링

1) SRT(Shortest Remaining Time) 스케줄링

- 짧은 시간 순서대로 프로세스를 수행한다.
- 현재 CPU에서 실행 중인 프로세스의 남은 CPU 버스트 시간보다 더 짧은 CPU 버스트 시간을 가지는 프로세스가 도착하면 CPU가 선점된다.

2) Round Robin 스케줄링

- 시분할 시스템의 성질을 활용한 방법
- 일정 시간을 정하여 하나의 프로세스가 이 시간동안 수행하고 다시 대기 상태로 돌아간다.
- 그리고 다음 프로세스 역시 같은 시간동안 수행한 후, 대기한다. 이러한 작업을 모든 프로세스가 돌아가면서 진행하며, 마지막 프로세스가 끝나면 다시 처음 프로세스로 돌아와서 작업을 반복한다.
- 일정 시간을 Time Quantum(Time Slice)라고 부른다. 일반적으로 10 ~ 100msec 사이의 범위를 갖는다.
- 한 프로세스가 종료되기 전에 time quantum이 끝나면 다른 프로세스에게 CPU를 넘겨주기 때문에 선점형 스케줄링의 대표적인 예시다.

3) Multi-level Queue 스케줄링

- 프로세스를 그룹으로 나누어, 각 그룹에 따라 Ready Queue(준비 큐)를 여러 개 두며, 각 큐마다 다른 규칙을 지정할 수도 있다.(ex. 우선순위, CPU 시간 등)
- 즉, 준비 큐를 여러 개로 분할해 관리하는 스케줄링 방법이다.
- 프로세스들이 CPU를 기다리기 위해 한 줄로 서는 게 아니라 여러 줄로 선다.

![img](https://user-images.githubusercontent.com/34755287/53879673-5e979880-4052-11e9-9f9b-e8bfec7c9be6.png)

4) Multi-level feedback Queue 스케줄링

- 기본 개념은 Multi-level Queue와 동일하나, 프로세스가 하나의 큐에서 다른 큐로 이동 가능하다는 점이 다르다.

![img](https://user-images.githubusercontent.com/34755287/53879675-5f302f00-4052-11e9-86a2-c02ee03bac64.png)

- 위 그림에서 모든 프로세스는 가장 위의 큐에서 CPU의 점유를 대기한다. 이 상태로 진행하다가 이 큐에서 기다리는 시간이 너무 오래 걸린다면 **아래의 큐로 프로세스를 옮긴다.** 이와 같은 방식으로 대기 시간을 조정할 수 있다. 
- 만약, 우선순위 순으로 큐를 사용하는 상황에서 우선순위가 낮은 아래의 큐에 있는 프로세스에서 starvation 상태가 발생하면 이를 우선순위가 높은 위의 큐로 옮길 수도 있다.
- 대부분의 상용 운영체제는 여러 개의 큐를 사용하고 각 큐마다 다른 스케줄링 방식을 사용한다. 프로세스의 성격에 맞는 스케줄링 방식을 사용하여 최대한 효율을 높일 수 있는 방법을 선택한다.

### 비선점형 스케줄링

1) FCFS(First Come First Server)

- 준비 큐에 먼저 도착한 프로세스가 먼저 CPU를 점유하는 방식이다.
- CPU를 할당받으면 CPU 버스트가 완료될 때까지 CPU를 반환하지 않으며, 할당되었던 CPU가 반환될 때만 스케줄링이 이루어진다.

<img src="https://user-images.githubusercontent.com/33534771/89703489-500b8a00-d986-11ea-9b6c-27cca4ea1016.png" width="300" height="300"/>

![img](https://user-images.githubusercontent.com/34755287/53879661-5d666b80-4052-11e9-8453-bad918a563ef.png)

표는 3개의 프로세스와 각 프로세스가 CPU를 사용한 시간(Burst Time)을 나타낸다.

이를 간트 차트로 표현하면 그림과 같다. (도착 시간은 모두 0초라고 가정한다.)

평균 대기 시간은 아래와 같다.

- Average Waiting Time = 0 + 24 + 27 / 3 = 17msec

만약, 프로세스가 들어온 순서가 P3, P2, P1이라면 간트 차트는 아래와 같이 바뀐다.

![img](https://user-images.githubusercontent.com/34755287/53879665-5d666b80-4052-11e9-8ad5-8639b73b13ac.png)

- Average Waiting Time = 3 + 6 + 0 / 3 = 3msec

두 경우에서 모든 프로세스가 끝난 시간은 30msec로 같지만, 평균 대기 시간으로 봤을 때는 위의 예제는 17msec이고 아래는 3msec로 차이가 난다. **즉, 들어온 순서로 수행한다고 해서 반드시 효율적인 것은 아니다.**

위의 예제처럼 `P1, P2, P3` 순서로 들어온 경우를 **Convoy Effect** 라고 한다. 

이는 CPU 시간을 오래 사용하는 프로세스가 먼저 수행되는 동안 나머지 프로세스들은 그만큼 오래 기다리는 것을 뜻한다. P1이 수행되는 동안 P2, P3는 오래 기다리게 된다. 

#### 단점

- Convoy Effect 발생 : 소요 시간이 긴 프로세스가 짧은 프로세스보다 먼저 도착해서 뒤에 프로세스들이 오래 기다려야 하는 현상



2) SJF(Shortest-Job-First)

- 다른 프로세스가 먼저 도착했더라도 CPU 버스트가 짧은 프로세스에게 CPU를 먼저 할당하는 방식이다.
- 선점, 비선점 모두 가능하다.

<img src="https://user-images.githubusercontent.com/33534771/89703540-b7293e80-d986-11ea-9b11-0dabd8e5488f.png" width="300" height="300"/>

![img](https://user-images.githubusercontent.com/34755287/53879666-5d666b80-4052-11e9-93c2-86b725588403.png)

위의 간트 차트는 SJF를 사용했다. 평균 대기 시간은 아래와 같다.

- Average Waiting Time(AWT) = 3 + 9 + 16 + 0 / 4 = 7msec

이번에는 위의 표를 FCFS를 사용해 간트 차트로 나타내고 평균 대기 시간을 구해보자.

![img](https://user-images.githubusercontent.com/34755287/53879667-5d666b80-4052-11e9-8cd4-066aefcf3047.png)

- Average Waiting Time(AWT) = 0 + 6 + 14 + 21 / 4 = 10.25msec

SJF가 평균 대기 시간이 더 짧다. 수학적으로 증명되었으며, 어떠한 예제를 보더라도 SJF의 AWT가 짧다는 것을 알 수 있을 것이다. 



SJF가 가장 효율적인 CPU 스케줄링 방법 같지만, 매우 **비현실적**이다. 왜냐하면 컴퓨터 환경에서는 프로세스의 CPU 점유 시간(Burst time)을 알 수 없다. 한 프로세스가 실행 중에는 많은 변수가 존재하기 때문에 CPU 점유 시간을 알려면 실제로 수행하여 측정하는 수밖에 없다. 실제 측정한 시간으로 예측하여 SJF를 사용할 수도 있지만, 이는 오버헤드가 매우 큰 작업으로 잘 사용되지 않는다.



3) Priority

- 우선순위가 높은 프로세스가 먼저 선택되는 스케줄링 알고리즘이다.
- 우선순위는 정수값으로 나타내며, 작은 값이 우선순위가 높다.(Unix/Linux 기준)
- 선점, 비선점 모두 가능하다.

<img src="https://user-images.githubusercontent.com/33534771/89703556-e344bf80-d986-11ea-87f1-74994cc47510.png" width="300" height="300"/>

![img](https://user-images.githubusercontent.com/34755287/53879671-5e979880-4052-11e9-84d3-524270cdc920.png)

우선순위가 낮은 순서대로 수행한 모습을 간트 차트로 나타냈다.

- Average Waiting Time(AWT) : 0 + 6 + 16 + 18 + 1 / 5 = 8.2msec

우선순위를 정하는 방법은 크게 내부적인 요소와 외부적인 요소로 나뉜다.

- Internal : time limit, memory requirement, I/O to CPU burst(I/O 작업은 길고, CPU 작업은 짧은 프로세스 우선) 등
- External : amout of funds being paid, political factors 등
- 단점 : Starvation(기아) 현상

CPU의 점유를 오랜 시간 동안 하지 못하는 현상을 의미한다. Priority 스케줄링 방식에서 우선순위가 매우 낮은 프로세스가 ready queue에서 대기하고 있다고 가정해보자.

이 프로세스는 아무리 오래 기다려도 CPU를 점유하지 못할 가능성이 매우 크다. 실제 컴퓨터 환경에서는 새로운 프로세스가 자주 ready queue에 들어온다. 이러한 프로세스가 모두 우선순위가 높은 상태라면 이미 기다리고 있던 우선순위가 낮은 프로세스는 하염없이 기다리고만 있는 상태로 남아있을 수 있다. 



이를 해결하는 방법 중 하나는 **aging**이 있다. ready queue에서 기다리는 동안 일정 시간이 지나면 우선 순위를 일정량 높여주는 것이다. 그러면 우선순위가 매우 낮은 프로세스라 하더라도, 기다리는 시간이 길어질수록 우선순위도 계속 높아지므로 수행될 가능성이 커진다.



## 스케줄러의 종류

- 프로세스들은 자신이 종료될 때까지 수많은 큐들을 돌아다닌다. OS는 이 큐 안에 있는 프로세스 중 하나를 선택해야 하며, 이러한 일을 '스케줄러(Scheduler)'가 담당한다.
- 작업 큐(Job Queue) : 현재 시스템 내의 모든 프로세스의 집합
- 준비 큐(Ready Queue) : 현재 메모리 내에 있으면서 CPU를 할당받고 실행되기 위해 기다리는 프로세스의 집합
- 장치 큐(Device Queue) : 각각의 장치마다 서비스를 기다리며 줄 서 있는 프로세스의 집합



#### 장기 스케줄러(Long-term Scheduler)

- 메모리는 한정되어 있는데 많은 프로세스들이 메모리에 한꺼번에 올라올 경우, 대용량 메모리(디스크)에 임시로 저장한다. 이 Pool(디스크) 내의 저장되어 있는 프로세스 중 어떤 순서로 프로세스를 메모리에 적재할지 결정한다.

- **메모리와 디스크 사이의 스케줄링**을 담당한다. 따라서 상대적으로 호출되는 빈도가 적다.
- 즉, 디스크와 같은 저장 장치에 작업들을 저장해 놓고 필요할 때 실행할 작업을 Job Queue에서 꺼내 Ready Queue를 통해서 메인 메모리에 적재한다.
- degree of Multiprogramming 제어(메모리에 여러 프로그램이 올라가는 것. 즉, 몇 개의 프로그램이 올라갈 것인지를 제어)
- 프로세스의 상태
  - 시작 상태(New) -> 준비 상태(Ready)
  - Running(or Ready) -> Terminated



#### 단기 스케줄러(Short-term Scheduler)

- **CPU와 메모리 사이의 스케줄링**을 담당한다. 따라서 장기 스케줄러에 비해 매우 많이 호출된다.
- 우선, CPU에게 필요한 데이터를 확보해주고 메모리에 있는 프로세스 중 하나를 선택해서 CPU를 할당한다.
- 즉, Ready Queue에 존재하는 프로세스 중 어떤 프로세스를 running 시킬지 결정한다.
- Ready Queue에 있는 프로세스 중 **먼저 도착한 프로세스에게 CPU를 할당**한다.(=디스페처)
- 프로세스의 상태 : ready -> running -> waiting -> ready



#### 중기 스케줄러(Medium-term Scheduler)

- 시분할 시스템에서 추가로 사용하며, 메모리에 대한 가중을 완화시켜주기 위해 중기 스케줄러 도입.

- CPU를 차지하기 위한 경쟁이 심해질 때, 우선순위가 낮은 프로세스들을 잠시 제거한 뒤, 나중에 경쟁이 완화되었을 때 다시 디스크에서 메모리로 불러와 중단되었던 지점부터 실행시킨다. (Swapping)
- 즉, 프로세스들이 서로 CPU를 차지하려고 경쟁이 심해지면 Swapping 기법을 활용하여 메모리를 관리함으로써 너무 많은 프로그램이 동시에 올라가는 것을 조절한다.

- 프로세스의 상태 : ready -> suspended



swap out : 메모리에서 디스크로 잠시 나가는 상태

swap in : 디스크에서 메모리로 다시 들어오는 상태



#### 정리

장기 스케줄러와 단기 스케줄러의 가장 큰 차이점은 실행 빈도이다.

프로세스는 빠르게 실행되고 이러한 프로세스들을 처리하기 위해 즉, 프로세스들 간의 우선순위를 정하기 위해 단기 스케줄러가 동작한다. 여기서 스케줄링의 시간이 지연되면 안되기 때문에 단기 스케줄러는 상당히 빨라야 하고 호출 빈도수가 많다.



그에 반해 시스템에 새로운 작업이 생성되어 들어오는 것은 분 단위로 프로세스의 함수가 실행되는 시간에 비해 무지 길다. 추가로 시스템에서 이탈하는 프로세스도 관리하지만, 장기 스케줄러는 단기 스케줄러보다 호출 빈도수가 매우 적다. 그리고 장기 스케줄링은 스케줄링 시간이 꽤 걸리더라도 신중하게 프로세스를 선택한다. 

만약, 장기 스케줄링이 I/O 프로세스나 CPU 중심 프로세스 중 한쪽으로 편중해서 프로세스를 받아온다면 ready Queue, device Queue 한쪽에 프로세스가 집중되어 버리게 되어 단기 스케줄러의 균형도 붕괴된다.



## 시스템 콜(System Call)

먼저, 커널 모드와 사용자 모드에 대해서 알아보자.

1) 커널 모드

프로그램 카운터가 운영체제가 존재하는 부분을 가리키고 있다면, 현재 운영체제의 코드를 수행 중이며 CPU가 커널 모드에서 수행 중이라고 한다.

2) 사용자 모드

프로그램 카운터가 사용자 프로그램이 존재하는 메모리 위치를 가리킬 경우, 사용자 프로그램을 수행 중이며 CPU가 사용자 모드에서 수행 중이라고 한다.

- 일반 명령 : 메모리에서 자료를 읽어와서 CPU에서 계산하고 결과를 메모리에 쓰는 일련의 명령들. 모든 프로그램이 수행할 수 있음. (사용자 모드)
- 특권 명령 : 보안이 필요한 명령으로 입출력 장치, 타이머 등 각종 장치를 접근하는 명령.(커널 모드)
- CPU 내에 모드 비트를 두어 두 명령을 수행한다.
- 사용자 프로그램이 디스크의 파일을 접근하거나, 화면에 결과를 출력하는 등의 작업이 필요한 경우가 있다. 하지만, 이러한 작업은 특권 명령의 수행을 필요로 한다. 
- 이와 같은 경우, 사용자 프로그램은 스스로 특권 명령을 수행할 수 없으므로 운영체제에게 특권 명령의 대행을 요청한다. 이러한 서비스 요청은 시스템 콜이라고 부른다. (즉, 특권 명령의 대행을 요청하여 사용자 프로그램이 커널 영역의 기능을 수행하게 해준다.)

**디스크에서 자료를 읽어오는 시스템 콜이라고 가정!**

1. 사용자 프로그램이 시스템 콜을 하게 되면 운영체제는 자신의 커널 영역에 정의된 시스템 콜 처리 코드를 수행한다.
2. CPU가 컨트롤 레지스터를 세팅해 디스크 컨트롤러에게 데이터를 읽어오라고 명령한다.
3. 디스크 컨트롤러는 디스크로부터 데이터를 읽어와서 자신의 로컬 버퍼에 저장한다.
4. 작업이 완료되면 디스크 컨트롤러가 CPU에게 인터럽트를 발생시켜 입출력 작업이 완료되었음을 통지한다.

통상적으로 시스템 콜은 여러 종류의 기능으로 나뉘어져 있다. 각 시스템 콜에는 번호가 할당되고 시스템 콜 인터페이스는 이러한 번호에 따라 인덱스되는 테이블을 유지한다. 아래 그림은 open() 시스템 콜을 호출했을 때, 운영체제에서 어떻게 처리되는지를 보여준다.

![img](https://t1.daumcdn.net/cfile/tistory/25333241535CCEE810)

필요한 기능이나 시스템 환경에 따라 시스템 콜이 발생할 때, 좀 더 많은 정보가 필요할 수도 있다. 그러한 정보가 담긴 매개변수를 운영체제에 전달하기 위해서는 대략 3가지 정도의 방법이 있다.

1) 매개변수를 CPU 레지스터 내에 전달한다. 이 경우에 매개변수의 갯수가 CPU 내의 레지스터 갯수보다 많을 수 있다.
2) 위와 같은 경우에 매개변수를 메모리에 저장하고 **메모리의 주소가 레지스터에 전달**된다. (아래 그림 참고)
3) 매개변수는 프로그램에 의해 **스택으로 전달**될 수도 있다.

![img](https://t1.daumcdn.net/cfile/tistory/27118142535CCF060A)



2,3번은 전달되는 **매개변수의 갯수나 길이에 제한이 없기 때문에** 몇몇 운영체제에서 선호하는 방식이다.



### 시스템 콜의 유형

5가지의 범주로 나눌 수 있다.

1. 프로세스 제어 : 프로세스 특권 모드를 사용해 직접적으로 프로세스 제어가 가능
2. 파일 조작 : 파일을 생성하거나 삭제, 관리 등
3. 장치 관리 : 장치 요구 및 장치 해제, 읽기, 쓰기, 재배치 등
4. 정보 유지 : 시간과 날짜의 설정과 획득, 시스템 자료의 설정과 획득
5. 통신 : 통신 연결의 생성 및 제거, 메시지의 송수신, 상태 정보 전달 등



## 인터럽트

프로그램을 실행하는 도중에 예기치 않은 상황이 발생할 경우 현재 실행 중인 작업을 즉시 중단하고 발생된 상황을 우선 처리한 후 실행 중이던 작업을 복귀하여 계속 처리하는 것으로 지금 수행 중인 일보다 더 중요한 일이 발생하면 그 일을 먼저 처리하고나서 하던 일을 계속하는 것을 의미한다.

### 인터럽트의 종류

1. 하드웨어 인터럽트(일반적인 인터럽트)
   - 하드웨어 컨트롤러가 CPU의 서비스를 요청하기 위해 발생시키는 인터럽트

<img src="https://k.kakaocdn.net/dn/bwovTQ/btqvBp24GS1/AxuTAkEaZaQwobodqeEfQk/img.png" alt="img" style="zoom:50%;" />

<center>하드웨어 인터럽트 발생 과정</center>

2. 소프트웨어 인터럽트(Trap:트랩)

1) 예외 상황(Exception) 

- 프로그램이 허용되지 않은 연산을 수행하려고 할 때, 자동적으로 발생한다. 운영체제는 예외 상황이 발생했을 때, CPU의 제어권을 획득해 이 상황에 대한 조치를 취한다.
- ex) 0으로 나누는 연산, 자신의 주소 공간을 넘어서는 메모리 참조 등
- 예외 상황에 대한 처리 루틴을 자신의 코드 영역에 가지고 있음

2) 시스템 콜(System Call) 

- 사용자 프로세스가 운영체제의 서비스를 요청하기 위해 커널의 함수를 호출하는 것이다. 
- 사용자 프로세스가 직접 특권 명령을 수행할 수 없으므로 특권 명령을 수행하려 할 때, 시스템 콜을 사용한다. 

<img src="https://k.kakaocdn.net/dn/QnqLh/btqvABC7Ea2/sflPVirxNdWXOiQkc8CQz1/img.png" alt="img" style="zoom:50%;" />

<center>소프트웨어 인터럽트 발생 과정</center>

시스템 콜이나 예외 상황은 모두 사용자 프로세스로부터 CPU의 제어권이 운영 체제에게 이양되어 처리되는데 이 과정에 인터럽트 라인을 세팅하여 인터럽트를 발생시킨 후 제어권이 넘어가게 되므로 이들도 넓은 의미에서는 인터럽트의 범주에 포함시킨다. **단지 인터럽트를 발생시키는 주체가 하드웨어 장치가 아닌 소프트웨어이므로 이들을 소프트웨어 인터럽트라고 부른다.**



### 인터럽트 발생 처리 과정

A 프로그램이 CPU를 할당받고 명령을 수행하고 있는데 인터럽트가 발생하면 A는 현재 수행중인 명령의 위치를 저장해놓는다. 그 후, 운영 체제 내부 코드인 인터럽트 처리 루틴으로 넘어가서 인터럽트 처리를 하고 다시 돌아와 A의 이전 작업 지점부터 수행을 계속 이어나가게 된다.



`그렇다면, 인터럽트가 발생했을 때 수행중이던 프로세스의 정보는 어디에 저장될까?`

-> 진행 중이던 A 프로세스의 정보는 프로세스 제어 블록(PCB: Process Control Block)에 저장한다. 그리고 인터럽트 처리를 모두 마치면 프로그램 A의 PCB에 저장된 주소를 복원시켜 원래 수행하던 일을 재개하게 된다. 



- 인터럽트 벡터(Interrupt Vector)
  - 여러가지 인터럽트에 대해 해당 인터럽트 발생시 처리해야 할 루틴의 주소를 보관하고 있는 테이블.
  - 일종의 함수를 가리키는 포인터

- 인터럽트 핸들러(Interrupt Handler)
  - 실제 인터럽트를 처리하기 위한 루틴으로 인터럽트 서비스 루틴이라고도 한다.
  - 운영체제 코드 부분에는 각종 인터럽트 별로 처리해야 할 내용이 이미 프로그램되어 있으며, 이 부분을 인터럽트 서비스 루틴 또는 인터럽트 핸들러라고 한다.

예를 들어, 입출력 관련 인터럽트가 발생한 경우, CPU는 인터럽트 라인을 통해 발생한 인터럽트를 확인한다. 인터럽트 벡터를 통해 해당 인터럽트 발생시 처리해야 할 루틴의 메모리 주소를 알아낸다. 주소를 통해 실제 수행되어야 할 코드가 담겨있는 루틴을 찾아가 상황에 맞는 처리를 진행한다.





## IPC(Inter-Process Communication)

IPC란, 하나의 컴퓨터 안에서 실행중인 서로 다른 프로세스 간에 발생하는 통신을 말한다.이러한 통신에서는 데이터의 불일치 문제가 발생할 수 있기 때문에 의사 소통기능과 함께 동기화를 보장해 주어야 한다.대표적인 예로는 다음과 같은 방식들이 있습니다.

- 메시지 전달 방식
    - 프로세스 간에 공유 변수를 일체 사용하지 않고 메시지를 주고받으면서 통신하는 방식이다. 두 프로세스의 주소 공간이 다르므로 메세지 전달을 직접할 수 없고, 커널이 대신 그 역할을 하게 된다. 메시지 전달 방식은 메시지의 전송 대상이 **다른 프로세스**인지 아니면 **메일 박스라는 일종의 저장 공간**인지에 따라 직접 통신과 간접 통신으로 나뉘게 된다.
    ![](https://i.imgur.com/ltF44ey.png)

- 공유 메모리 방식
    - 프로세스들이 주소 공간 일부를 공유하게 되는 방식이다. 서로 다른 프로세스들은 원칙적으로 독립적인 주소 공간을 사용하지만, 운영 체제를 통해 공유 메모리 사용하는 시스템 콜을 사용하여 서로 다른 프로세스들이 그들의 주소 공간 중 일부를 공유할 수 있도록 한다. 이러한 공유 메모리방식은 프로세스 간의 통신을 수월하게 할 수 있는 인터페이스를 제공하지만, 서로간의 데이터 일관성 문제가 유발 될 수 있다.
    ![](https://i.imgur.com/AmXGwwO.png)


## 메모리

## 페이징과 세그멘테이션
- 가상 메모리를 관리하는 기법
- 가상 메모리란 메모리에 로드된 즉, 실행중인 프로세스가 가상의 공간을 참조하여 마치 커다란 물리 메모리를 갖고 있는 것처럼 사용할 수 있도록 하는 것이다.
- 간단하게 말해 **실제 메모리 주소가 아닌 가상의 메모리 주소를 주는 방식**이다.

ex) 내가 실행하고자 하는 프로그램의 용량이 5GB인데, 메모리는 4GB이다. 어떻게 실행할까?

올리는 것도 문제이지만, 올린다고 하더라도 해당 프로그램이 실행될 때는 다른 작업은 아무것도 하지 못하게 된다. 이럴 때, 사용하는 기술이 바로 가상 메모리이다.

가상 메모리는 각 프로세스당 메인 메모리와 동일한 크기로 하나씩 할당된다. 그 공간은 보조기억장치 공간을 이용한다. 프로세스의 일부만 메모리에 로드하고 나머지는 보조기억장치에 두는 형태이다.

<img src="https://user-images.githubusercontent.com/33534771/89703730-5995f180-d988-11ea-9864-1870639e4e7a.png" />

이렇게 할당되면 메모리 관리 장치(MMU)에 의해 물리 주소로 변환되어 사용자가 메모리 맵핑이 어떻게 되는지 의식할 필요 없이 알아서 가상 메모리를 활용하여 작업한다.

### 페이징
- 프로세스의 주소 공간을 동일한 사이즈의 페이지 단위로 나누어 물리적 메모리에 불연속적으로 저장하는 방식
- 외부 단편화와 압축 작업을 해소하기 위함이다.
- 메모리는 'Frame'이라는 고정 크기로 분할되고, 프로세스는 'Page'라는 고정 크기로 분할된다.

> 하나의 프로세스는 연속적인 동작을 수행하는데, 이를 작은 조각으로 나누어 여러 곳에 흩어지게 된다면 정상적으로 동작할까?

메모리 상에서 여러 곳에 흩어진 프로세스를 수행하기 위해 MMU라는 메모리 관리 장치를 통해 논리 주소와 물리 주소를 나누어 사용함으로써 CPU를 속여야 한다.
즉, 실제 메모리는 전혀 연속적이지 않은데, CPU는 연속적으로 사용하고 있다는 것을 보장받으며 정상적으로 수행한다.

- 50byte 크기의 프로세스가 있다고 가정하고, 페이징의 크기는 10byte로 나눈다.

![img](https://user-images.githubusercontent.com/34755287/54821888-d9191700-4ce6-11e9-8b11-7af6fdbcbe06.png)

- 프로세스 P1은 5개의 페이지로 나눌 수 있다. 이를 메인 메모리 5곳에 나눠서 할당한다.
- CPU는 논리 주소로 프로그램이 설정한대로 연속적인 주소값으로 명령을 내리고 이는 메모리로 가기 전에 각 페이지의 실제 메모리 주소가 저장되어 있는 **테이블에서 물리 주소로 변경되어야 한다.**
- 프로세스를 정상적으로 사용하기 위해 MMU의 재배치 레지스터를 여러 개 사용해서 위의 그림과 같이 각 페이지의 실제 주소로 변경해준다. 이러한 여러 개의 재배치 레지스터를 '페이지 테이블'이라 한다.

[단점]
- 내부 단편화 문제의 비중이 늘어나게 된다.
- ex) Page 크기 : 1024B, 프로세스 A가 3172의 메모리를 요구한다면 3개의 페이지 프레임(1024x3 = 3072)을 구성하고도 100B가 남기 때문에 총 4개의 페이지 프레임이 필요하다. 마지막 페이지 프레임에는 924B의 여유 공간이 남게 되는 내부 단편화 문제가 발생한다.

### 세그멘테이션
- 프로세스를 서로 크기가 다른 논리적인 블록 단위인 '세그먼트(Segment)'로 분할하고 메모리에 배치하는 것을 말하며, 각 세그먼트의 크기는 일정하지 않다.
- 프로세스를 Code + Data + Stack 영역으로 나누는 것 역시 세그멘테이션의 모습이다. 물론, code, data, stack 각각 내부에서 더 작은 세그먼트로 나눌 수도 있다.
- 세그먼트를 메모리에 할당할 때는 페이지를 할당하는 것과 동일하다. 하지만, 테이블은 조금 다르다. 세그먼테이션을 위한 테이블은 **세그먼트 테이블**이라고 한다. 
- 세그먼트 테이블은 세그먼트 번호와 시작 주소, 세그먼트 크기를 엔트리로 갖는다. 
- 세그먼트에서 주소변환 역시, 페이징과 유사하다. 한 가지 주의할 점은 세그먼트의 크기는 일정하지 않기 때문에, 테이블에 **limit** 정보가 주어진다. 그리고 CPU에서 해당 세그먼트의 크기를 넘어서는 주소가 들어오면 인터럽트가 발생해서 해당 프로세스를 강제로 종료시킨다. 

![img](https://user-images.githubusercontent.com/34755287/57119448-47043400-6da5-11e9-95da-91cb808de992.png)

위 그림은 세그먼트 테이블과 프로세스가 할당된 메모리의 모습이다. 페이징 주소변환과 동일하게 d(변위 : 변하지 않는 값)는 논리주소와 물리주소가 동일하다. 물리주소 a는 **base[s]+d** 로 계산된다.

- 논리 주소(2, 100) -> 물리주소 4400번지
- 논리 주소(1, 500) -> limit이 400밖에 안되므로 인터럽트로 인해 프로세스 강제 종료(범위를 벗어남)

[단점]

- 외부 단편화 문제가 발생할 수 있다. 


## 단편화

### 외부 단편화
- 프로그램의 크기보다 분할의 크기가 작은 경우, 해당 분할이 비어있음에도 불구하고 프로그램을 적재하지 못하기 때문에 발생하는 메모리 공간을 말한다.
- 어떤 프로그램에도 배당되지 않은 빈 공간임에도 현재 상태에서 사용될 수 없는 작은 분할이다.


### 내부 단편화
- 프로그램의 크기보다 분할의 크기가 큰 경우, 해당 분할에 프로그램을 적재하고 남는 분할 내부의 메모리 공간을 말한다.
- 즉, 하나의 분할 내부에서 발생하는 사용되지 않는 메모리 조각이다.
- ex) 메모리 자유 분할 공간이 10,000B가 있고 프로세스 A가 9,999B를 사용하게 되면 2B가 남게 된다. 이러한 현상을 내부 단편화라고 한다.


### 압축(Compaction)
- 외부 단편화를 해소하기 위해 프로세스가 사용하는 공간들을 한쪽으로 몰아, 자유 공간을 확보하는 방법론이다.
- 단점 : 작업 효율이 좋지 않다.

## Swapping

- 메모리에 올라온 프로세스의 주소 공간 전체를 디스크의 스왑 영역으로 일시적으로 내려놓는 것이다. 메모리 공간을 확보하면 이후에 다른 프로세스의 메모리를 불러 들일 수 있다.
- 주의할 점은 프로세스가 종료되어 주소 공간을 디스크로 내쫓는 것이 아니라 특정한 이유로 **수행중인 프로세스의 주소공간을 일시적으로 메모리에서 디스크로 내려놓은 것을 의미**한다.
  - 스왑인 : 디스크 -> 메모리
  - 스왑 아웃 : 메모리 -> 디스크



**[연속 할당 방식]**

- 각각의 프로세스를 물리적 메모리의 연속적인 공간에 올리는 방식
  1. 고정 분할 방식
     - 물리적 메모리를 주어진 개수만큼의 영구적인 분할로 미리 나누고 각 분할에 하나의 프로세스를 적재하여 실행시키는 방식
     - 단편화 문제 발생
  2. 가변 분할 방식
     - 메모리에 적재되는 프로그램의 크기에 따라 분할의 크기, 개수가 동적으로 변하는 방식
     - 외부 단편화 발생



**[불연속 할당 방식]**

- 하나의 프로세스를 물리적 메모리의 여러 영역에 분산하여 적재하는 방식
  1. 페이징 
     - 프로세스를 동일한 크기의 페이지로 나눈다.
     - 내부 단편화 문제
  2. 세그멘테이션
     - 프로세스를 서로 다른 크기의 논리적 블록 단위인 세그멘테이션으로 나눈다.
     - 외부 단편화 문제



## 페이지 교체 알고리즘

- 가상 메모리는 `요구 페이징 기법`을 통해 필요한 페이지만 메모리에 적재하고 사용하지 않는 부분은 그대로 둔다.
- 하지만, 필요한 페이지만 올리더라도 메모리는 결국 가득 차게 되고, 올라와있던 페이지가 사용이 다 된 후에도 자리만 차지하고 있을 수 있다.
- 메모리가 가득차면, 추가로 페이지를 가져오기 위해서 안쓰는 페이지는 out하고 해당 공간에 현재 필요한 페이지를 in 시켜야 한다. 이때, 어떤 페이지를 out 시킬지 정해야 한다. 
- 수정이 되지 않는 페이지를 선택해야 좋다. 만약, 수정되면 메인 메모리에서 내보낼 때, 하드디스크에서 또 수정해야 하므로 시간이 오래걸린다.
- 이러한 상황에서 적절한 페이지 교체를 위해 페이지 교체 알고리즘이 존재한다.



#### 가상 메모리

- 운영체제는 모든 프로그램에게 같은 크기의 메모리를 할당하지 않고 몇몇 프로그램에게 집중적으로 메모리를 할당한 후, 시간이 흐르면 이들에게서 메모리를 회수하여 다른 프로그램들에게 다시 집중적으로 할당하는 방식을 사용한다.
- CPU를 할당받아 당장 수행해야 할 부분만 메모리에 올려놓고 그렇지 않은 부분은 디스크의 스왑 영역에 내려놓았다가 다시 필요해지만 메모리에 올라가 있는 부분과 교체하는 방식을 사용한다. 이를 통해 프로그램이 자기 자신만 메모리를 사용하는 것과 같은 효과를 낸다.
- 가상 메모리는 프로세스마다 각각 0번지부터 시작하는 자기 자신만의 주소 공간을 가지게 되며, 이들 공간 중 일부는 물리적 메모리에 적재되고 일부는 디스크의 스왑 영역에 적재된다. -> **요구 페이징 기법**



**[요구 페이징]**

- 가상 메모리는 요구 페이징 기법을 통해 필요한 페이지만 메모리에 적재하고 사용하지 않는 부분은 그대로 둔다.
- 특정 페이지에 대해 CPU의 요청이 들어온 후에야 해당 페이지를 메모리에 적재하며, 한번도 접근되지 않은 페이지는 물리적 메모리에 적재되지 않는다.
- 이를 통해 메모리 사용량이 감소하고 프로세스 전체를 메모리에 올리는데 들었던 입출력 오버헤드가 감소하며 응답시간이 단축된다.
- 유효, 무효 비트를 두어 각 페이지가 메모리에 존재하는지 표시한다.
  - 유효 : 페이지가 메모리에 적재됨.
  - 무효 : 페이지가 현재 메모리에 없는 경우이거나 그 페이지가 속한 주소 영역을 프로세스가 사용하지 않는 경우
- CPU가 참조하려는 페이지가 현재 메모리에 올라와 있지 않아 **무효**로 세팅되어 있는 경우를 `'페이지 부재'`가 일어났다고 한다.



**[페이지 교체]**

- 페이지 부재가 발생하면 요청된 페이지를 디스크에서 메모리로 읽어와야 한다. 이때 물리적 메모리에 빈 프레임이 존재하지 않을 수 있다.
- 이 경우, 메모리에 올라와있는 페이지 중 하나를 디스크로 쫓아내고 메모리에 빈 공간을 확보하는 작업이 필요하다. 이를 페이지 교체라고 한다.
- 페이지 교체를 할 때, 어떤 프레임에 있는 페이지를 쫓아낼 것인지 결정하는 알고리즘을 페이지 교체 알고리즘이라 하며, 목표는 페이지 부재율을 최소화하는 것이다. 

<u>*페이지 부재 발생 -> 새로운 페이지를 할당해야 한다. -> 현재 할당된 페이지(메모리에 올라와있는 페이지) 중 어떤 것을 교체할지 결정하는 방법*</u>



#### 페이지 교체 알고리즘

1) FIFO(First in First out) 알고리즘

- 페이지 교체시 메모리에 먼저 올라온 페이지를 먼저 내보내는 알고리즘
- 가장 간단한 방법으로, 특히 초기화 코드에서 적절한 방법이다. 
- 단점
  - 페이지의 향후 참조 가능성을 고려하지 않고, 물리적 메모리에 들어온 순서대로 내쫓을 대상을 선정하기 때문에 비효율적인 상황이 발생할 수 있다.
  - 빌레이디의 모순 현상이 발생할 수 있다. (페이지 프레임 수가 많으면(메모리 증가) 페이지 부재수가 줄어드는 것이 일반적이지만, 페이지 프레임 수를 증가시켰음에도 페이지 부재가 더 많이 일어나는 현상을 의미한다.)

![img](https://camo.githubusercontent.com/250c0a33495ccda3dcaaa1c2b4d79ec22bf07da3/68747470733a2f2f696d67312e6461756d63646e2e6e65742f7468756d622f523132383078302f3f73636f64653d6d746973746f727926666e616d653d68747470732533412532462532466b2e6b616b616f63646e2e6e6574253246646e253246565143474b253246627471754a7571526b79532532464c62334e6777486b427665303859685a704c6b713331253246696d672e706e67)



2) 최적 페이지 교체 알고리즘(Optimal Page Replacement)

- 앞으로 가장 오랫동안 사용하지 않은 페이지를 교체하는 방법
- 미래에 어떤 페이지가 어떠한 순서로 참조될지 미리 알고있다는 전제하에 알고리즘을 운영하므로 현실적으로 구현이 어렵다.
- 페이지 부재율이 가장 낮은 효율적인 알고리즘이다.

![img](https://t1.daumcdn.net/cfile/tistory/265B26335916A03F39)



3) LRU 알고리즘(Least Recently Used)

- 최근에 사용하지 않은 페이지를 가장 먼저 내보내는 방법
- 최근에 사용하지 않았으면, 나중에도 사용되지 않을 것이라는 아이디어에서 나왔다.
- OPT의 경우 미래 예측이지만, LRU의 경우는 과거를 보고 판단하므로 실질적으로 사용이 가능한 알고리즘 (실제로도 최근에 사용하지 않은 페이지는 앞으로도 사용하지 않을 확률이 높다.)
- OPT보다는 페이지 결함이 더 일어날 수 있지만, **실제로 사용할 수 있는 페이지 교체 알고리즘에서는 가장 좋은 방법 중 하나이다.**

![img](https://camo.githubusercontent.com/3da1359d56b8c12c4da96d937554b8375d0cac1f/68747470733a2f2f696d67312e6461756d63646e2e6e65742f7468756d622f523132383078302f3f73636f64653d6d746973746f727926666e616d653d68747470732533412532462532466b2e6b616b616f63646e2e6e6574253246646e2532466e43676333253246627471754757395655726d25324678544b6e564b504f56517553586d4175526568537731253246696d672e706e67)



4) LFU 알고리즘(Least Frequently Used) 

- 페이지의 참조 횟수로 교체할 페이지를 결정하는 방법
- 즉, 물리적 메모리 내에 존재하는 페이지 중에서 과거에 참조 횟수가 가장 적었던 페이지를 내쫓고 그 자리에 새로 참조될 페이지를 적재한다.
- 단점
  - 시간에 따른 페이지 참조의 변화를 반영하지 못하고 LRU보다 구현이 복잡하다.

- LRU, LFU 알고리즘은 페이지의 최근 참조 시각 및 참조 횟수를 소프트웨어적으로 유지해야 하므로 알고리즘의 운영에 오버헤드가 발생한다.

5) 클럭 알고리즘(NRU : Not Recently Used, NUR : Not Used Recently)

- 하드웨어적인 지원을 받아 LRU와 LFU 알고리즘에서 발생한 시간적인 오버헤드를 줄인 방식이다.
- LRU를 근사시킨 알고리즘이라고 하며, 오랫동안 사용하지 않은 페이지 중 하나를 교체한다.
- 최근에 참조되지 않은 페이지를 교체 대상으로 선정하는 측면에서 LRU와 비슷하지만 교체되는 페이지의 참조 시점이 가장 오래됐다는 것을 보장하지 못한다.
- 참조 비트(Reference Bit)와 변형 비트(Modified Bit, Dirty Bit)를 사용한다.
  - 참조 비트
    - 페이지가 참조될 때, 1로 자동 세팅된다. 
    - 페이지가 참조되지 않았을 때는 0이 되며 페이지는 교체한다.
  - 변형 비트
    - 페이지 내용이 변경되었을 때, 1로 지정한다.
    - 페이지 내용이 변경되지 않았을 때는 0으로 지정한다.

- 이 알고리즘은 하드웨어의 자원으로 동작하기 때문에 LRU에 비해 교체 페이지의 선정이 훨씬 빠르게 결정된다.
- 따라서 대부분의 시스템에서 페이지 교체 알고리즘으로 클럭 알고리즘을 채택한다.



## 파일 시스템

## 동기와 비동기

동기/비동기의 주는 "요청한 작업의 완료 여부를 누가 신경을 쓰느냐"이다.

요청한 작업이 완료되어 반환을 기다리거나 반환을 받아도 작업이 완료가 되었는지 계속해서 여부를 물어보며 확인하는 것이 **동기**이다.

이와 반대로 요청한 작업이 완료가 되었는지 안되었는지 신경을 쓰지 않는게 **비동기**이다.



그렇다면 비동기에서는 종료를 누가 신겨을 쓰는 것일까?

바로 호출시에 같이 넘기는 **Callback** 함수이다. 호출되는 함수에게 Callback을 넘기게 되고 이 작업이 완료되었을 때, **Callback**이 실행되는 것으로 작업의 완료를 알린다.



- 동기 방식은 설계가 매우 간단하고 직관적이지만 결과가 주어질 때까지 아무것도 못하고 대기해야 한다는 단점이 있다.
- 반면, 비동기 방식은 동기보다 복잡하지만 결과가 주어지는데 시간이 걸리더라도 그 시간 동안 다른 작업을 할 수 있으므로 자원을 효율적으로 사용할 수 있다는 장점이 있다.



<!-- 해야할 일이 빨래, 설거지, 청소 3가지가 있다고 가정해보자.

- 이 일들을 동기적으로 처리한다면 빨래를 하고 설거지를 한 뒤, 청소를 한다. 
- 비동기적으로 일을 처리한다면 빨래하는 업체에게 빨래를 시킨다. 설거지 대행 업체에 설거지를 시키고, 청소 대행 업체에 청소를 시킨다. 셋 중 어떤 것이 먼저 완료될지는 알 수 없다. 일을 모두 마친 업체는 나에게 알려줄 것이니 나는 다른 작업을 할 수 있다.
  - 이 때는 백그라운드 스레드에서 해당 작업을 처리하는 경우의 비동기를 의미한다.

이제 비유가 아닌 개념적으로 설명해보자.

- 동기(synchronous : 동시에 일어나는)
  - 동시에 일어난다는 뜻이다. 요청과 그 결과가 동시에 일어난다는 약속이다.
  - 바로 요청을 하면 시간이 얼마가 걸리던지 요청한 자리에서 결과가 주어져야 한다.
  - 요청과 결과가 한 자리에서 동시에 일어난다.
  - A노드와 B노드 사이의 작업 처리 단위(transaction)를 동시에 맞추겠다.
- 비동기(Asynchronous : 동시에 일어나지 않는)
  - 동시에 일어나지 않는다를 의미한다. 요청과 결과가 동시에 일어나지 않을 것이라는 약속이다.
  - 요청한 그 자리에서 결과가 주어지지 않는다.
  - 노드 사이의 작업 처리 단위를 동시에 맞추지 않아도 된다. -->


## Blocking Non-blocking

Blocking/Non-Blocking의 관심사는 "호출된 함수가 바로 제어권을 넘겨 주느냐"이다.

#### Blocking I/O

호출된 함수가 자신의 작업을 모두 끝낼때까지 제어권을 가지고 있어 호출한 함수가 대기하도록 만든다.



#### NonBlocking I/O

호출된 함수가 바로 return 해서 호출한 함수에게 제어권을 주어 다른 일을 할 수 있게 한다.

호출되는 함수가 바로 리턴하느냐 마느냐가 중점이다.



NonBlocking & Sync와 Blocking & Async에 대해 알아보도록 하자.

1) NonBlocking & Sync

NonBlocking은 바로 return을 해서 **제어권을 준다**고 했고, Sync 작업 완료 여부를 **호출한** 쪽에서 신경 쓴다. 그림을 보면 호출하고 바로 반환이 되어 다른 일을 수행한다.

이후에 작업이 완료되었는지 계속 물어보는 일을 추가로 수행하는 것이 NonBlocking & Sync이다.

즉, NonBlocking이 제어권을 바로 return 해주면 Sync는 제어권을 받아서 작업 완료 여부를 호출한 쪽에서 계속 물어보며 추가로 일을 수행하게 되는 것이다.



2) Blocking & Async

Blocking은 작업이 완료될 때까지 제어권을 호출된 쪽에서 가지고 있고, Async는 작업의 완료 여부를 호출된 쪽에서 신경 쓴다. 

어차피 제어권이 없는 상태에서 결과만 기다리는 Blocking & Sync와 차이가 없다. 이 방식은 특별한 장점이 없어서 일부로 사용할 필요는 없다.

보통 NonBlocking & Async 방식을 쓰는 데 그 과정 중 하나라도 Blocking이 포함되면 의도치 않게 Blocking & Async로 작동한다고 한다. 



## 캐시의 지역성

#### 캐시의 지역성 원리

- 캐시 메모리는 속도가 빠른 장치와 느린 장치 간의 속도차에 따른 병목 현상을 줄이기 위한 범용 메모리이다. 
- 이러한 역할을 수행하기 위해서는 'CPU가 어떤 데이터를 원할 것인가'를 어느정도 예측할 수 있어야 한다.
- 캐시의 성능은 작은 용량의 캐시 메모리에 CPU가 이후에 참조할, 쓸모있는 정보가 어느정도 들어있느냐에 따라 좌우되기 때문이다.



이때, 적중률(Hit Rate)을 극대화시키기 위해 `데이터 지역성(Locality)의 원리` 를 사용한다. 지역성의 전제조건으로 프로그램은 모든 코드나 데이터를 균등하게 접근하지 않는다는 특성을 기본으로 한다. 

즉, Locality란 기억 장치 내의 정보를 균일하게 접근하는 것이 아닌 어느 한 순간에 특정 부분을 집중적으로 참조하는 특성이다. 데이터의 지역성은 대표적으로 시간 지역성(Temporal Locality)과 공간 지역성(Spatial Locality)으로 나뉜다.

- 시간 지역성 : 최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성
- 공간 지역성 : 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성



#### Cache line

캐시(Cache)는 프로세서 가까이에 위치하면서 빈번하게 사용되는 데이터를 보관하는 장소이다. 하지만, 캐시가 아무리 가까이 있더라도 찾고자 하는 데이터가 어느 곳에 저장되어 있는지 몰라 모든 데이터를 순회해야 한다면 시간이 오래 걸려 비효율적이다. 즉, 캐시에 목적 데이터가 저장되어 있다면 바로 접근하여 출력할 수 있어야 캐시의 의미가 지켜진다.



그렇기 때문에 캐시에 데이터를 저장할 때, 특정 자료구조를 사용하여 **묶음**으로 저장하게 되는데 이를 **캐싱 라인**이라고 한다. 다양한 주소에 있는 데이터를 사용하므로 빈번하게 사용하는 데이터의 주소는 흩어져 있다. 따라서 캐시에 저장하는 데이터에는 데이터의 메모리 주소 등을 기록해둔 태그를 달아놓을 필요가 있다. 이러한 태그들의 묶음을 캐싱라인이라고 하며, 메모리로부터 가져올 때도 캐싱 라인을 기준으로 가져온다. 대표적으로 3가지 종류의 방식이 있다.

1. Full Associative
2. Set Associative
3. Direct Map





---

###### 참고
- [[OS] 동기와 비동기](https://k39335.tistory.com/34)
- [동기와 비동기의 개념과 차이](https://private.tistory.com/24)
- [#3 인터럽트의 원리 - 운영체제와 정보기술의 원리](https://real-dongsoo7.tistory.com/m/93?category=784608)
- [규글님 Github](https://github.com/gyoogle/tech-interview-for-developer/blob/master/Computer%20Science/Operation%20System/Interrupt.md)
- [운영체제 04 : 시스템 콜](https://luckyyowu.tistory.com/133)
- [시스템 인터럽트, 시스템 콜](https://luckyyowu.tistory.com/2)
- [[운영체제(OS)] 11. 모니터](https://velog.io/@codemcd/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9COS-11.-%EB%AA%A8%EB%8B%88%ED%84%B0)
- [Blocking, NonBlocking](https://heecheolman.tistory.com/48)
- [[운영체제] 스케줄러](https://kim6394.tistory.com/177)
- [가상메모리-02-페이지 교체 알고리즘](https://eunhyejung.github.io/os/2018/07/24/operatingsystem-study15.html)
- [규글님 - 페이지 교체 알고리즘](https://github.com/gyoogle/tech-interview-for-developer/blob/master/Computer%20Science/Operation%20System/Page%20Replacement%20Algorithm.md)